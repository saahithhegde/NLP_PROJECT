{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-PART2ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1wL6s8fvq0mGT9aJVX_cEL1Vyp2xIS3mm",
      "authorship_tag": "ABX9TyNIhqgcgzLwdyxx6S3coY7v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saahithhegde/NLP_PROJECT/blob/master/NLP_PART2ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP3dc212Yk3w",
        "colab_type": "code",
        "outputId": "d9b0d0e7-4ef8-493d-9c52-58eefb9a09ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd "
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDpEr4HsXz7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install allennlp==1.0.0rc1 allennlp-models==1.0.0rc1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYiM8t_HYU07",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "abcd7ff7-aac3-44b1-91d5-9d993ad1c39c"
      },
      "source": [
        "from allennlp.predictors.predictor import Predictor\n",
        "import allennlp_models.coref\n",
        "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\")"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-04 21:10:54 WARNING: Did not use initialization regex that was passed: _context_layer._module.weight_hh.*\n",
            "2020-05-04 21:10:54 WARNING: Did not use initialization regex that was passed: _context_layer._module.weight_ih.*\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBBvjaE_ZxlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install stanza; note that the prefix \"!\" is not needed if you are running in a terminal\n",
        "!pip install stanza==1.0.0\n",
        "\n",
        "# Import stanza\n",
        "import stanza"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUJS-Vl-Zxvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!echo \"Downloading CoreNLP...\"\n",
        "!wget \"http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip\" -O corenlp.zip\n",
        "!unzip corenlp.zip\n",
        "!mv ./stanford-corenlp-full-2018-10-05 ./corenlp\n",
        "\n",
        "# Set the CORENLP_HOME environment variable to point to the installation location\n",
        "import os\n",
        "os.environ[\"CORENLP_HOME\"] = \"./corenlp\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uulD9duyZx23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from stanza.server import CoreNLPClient"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvGaqaqfZx9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "client = CoreNLPClient(annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner','coref','depparse'], memory='18G', endpoint='http://localhost:9001')\n",
        "print(client)\n",
        "\n",
        "# Start the background server and wait for some time\n",
        "# Note that in practice this is totally optional, as by default the server will be started when the first annotation is performed\n",
        "client.start()\n",
        "import time; time.sleep(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZVBd6P0bMR3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "from nltk.corpus import wordnet as wn\n",
        "from spacy import displacy\n",
        "from IPython.core.display import display, HTML\n",
        "from nltk.corpus import wordnet as wn\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#Token Nlp has all the possible models | do not use it for sentence splitting\n",
        "TokenNLP = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "#sentence nlp is used to explicitly spit the sentences \n",
        "SentenceNLP = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"ner\"])\n",
        "SentenceNLP.pipeline\n",
        "\n",
        "#custom boundries\n",
        "def custom_sentencizer(doc):\n",
        "    for i, token in enumerate(doc[:-2]):\n",
        "        # Define sentence start if pipe + titlecase token\n",
        "        if token.text == \"I\":\n",
        "            doc[i].is_sent_start = False\n",
        "        if token.text == \"(;\":\n",
        "            doc[i].is_sent_start = False\n",
        "        if token.text == \"(;\":\n",
        "            doc[i+1].is_sent_start = False\n",
        "        if token.text == \"...\":\n",
        "            doc[i+1].is_sent_start = True\n",
        "        if token.text == \":\":\n",
        "            doc[i+1].is_sent_start = False\n",
        "        if token.text == \":\":\n",
        "            doc[i].is_sent_start = False\n",
        "        if token.text == \"the\":\n",
        "            doc[i].is_sent_start = False\n",
        "        if token.text == '.\"':\n",
        "            doc[i+1].is_sent_start = True\n",
        "        if token.text == '.\"':\n",
        "            doc[i].is_sent_start = False\n",
        "        if token.text == 'Inc.':\n",
        "            doc[i].is_sent_start = False\n",
        "        if token.text == 'Inc.':\n",
        "            doc[i+1].is_sent_start = False\n",
        "        if token.text == ', Inc.':\n",
        "            doc[i+1].is_sent_start = False         \n",
        "    return doc\n",
        "\n",
        "SentenceNLP.add_pipe(custom_sentencizer, before=\"parser\")\n",
        "SentenceNLP.pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krJFhj3cZyVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"During his keynote speech at the Macworld Expo on January 9, 2007, Jobs announced that Apple Computer, Inc. would thereafter be known as 'Apple Inc.', because the company had shifted its emphasis from computers to consumer electronics. This event also saw the announcement of the iPhone and the Apple TV. The company sold 270,000 iPhone units during the first 30 hours of sales, and the device was called 'a game changer for the industry'. Apple would achieve widespread success with its iPhone, iPod Touch and iPad products, which introduced innovations in mobile phones, portable music players and personal computers respectively. Furthermore, by early 2007, 800,000 Final Cut Pro users were registered.\"\n",
        "document = client.annotate(text)\n",
        "print(document)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWqMS13EZyTy",
        "colab_type": "code",
        "outputId": "603ba163-85d2-469d-b48a-6dbf15c8429c",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "#to upload a file\n",
        "from google.colab import files\n",
        "uploaded=files.upload()\n",
        "filename=None\n",
        "for name, data in uploaded.items():\n",
        "  filename=name\n",
        "\n",
        "print(filename)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b9c8cad4-c830-477a-9663-1a052ea82688\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-b9c8cad4-c830-477a-9663-1a052ea82688\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving 4.txt to 4.txt\n",
            "4.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1GYJLxdZySg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5708281a-3d09-4b36-b2d0-5bb33d9ceb6b"
      },
      "source": [
        "#when you have uploaded a file on the left\n",
        "name=input(\"enter file name if you uploaded directly from the left\")\n",
        "filename=name+\".txt\""
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "enter file name if you uploaded directly from the leftsamplefilefordemo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOOowa9lZyLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#to upload a file \n",
        "with open(filename,encoding='utf-8-sig') as fd:\n",
        "  lines = fd.read().splitlines()\n",
        "  while '' in lines:\n",
        "    lines.remove('')\n",
        "print(\"The Document is split by \\\\n and appends each paragraph to the list:\")\n",
        "print(lines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgKYoeYtZyJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#lines are always the paragraphs:\n",
        "lines=[]\n",
        "myinput=input(\"Please enter some data\")\n",
        "lines.append(myinput)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Vtvd6DlcwcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pronounlist=[\"all\",\"another\",\"any\",\"anybody\",\"anyone\",\"anything\",\"as\",\"aught\",\"both\",\"each\",\"each other\",\"either\",\"enough\",\"everybody\",\"everyone\",\"everything\",\"few\",\"he\",\"her\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"I\",\"idem\",\"it\",\"its\",\"itself\",\"many\",\"me\",\"mine\",\"most\",\"my\",\"myself\",\"naught\",\"neither\",\"no one\",\"nobody\",\"none\",\"nothing\",\"nought\",\"one\",\"one another\",\"other\",\"others\",\"ought\",\"our\",\"ours\",\"ourself\",\"ourselves\",\"several\",\"she\",\"some\",\"somebody\",\"someone\",\"something\",\"somewhat\",\"such\",\"suchlike\",\"that\",\"thee\",\"their\",\"theirs\",\"theirself\",\"theirselves\",\"them\",\"themself\",\"themselves\",\"there\",\"these\",\"they\",\"thine\",\"this\",\"those\",\"thou\",\"thy\",\"thyself\",\"us\",\"we\",\"what\",\"whatever\",\"whatnot\",\"whatsoever\",\"whence\",\"where\",\"whereby\",\"wherefrom\",\"wherein\",\"whereinto\",\"whereof\",\"whereon\",\"wherever\",\"wheresoever\",\"whereto\",\"whereunto\",\"wherewith\",\"wherewithal\",\"whether\",\"which\",\"whichever\",\"whichsoever\",\"who\",\"whoever\",\"whom\",\"whomever\",\"whomso\",\"whomsoever\",\"whose\",\"whosever\",\"whosesoever\",\"whoso\",\"whosoever\",\"ye\",\"yon\",\"yonder\",\"you\",\"your\",\"yours\",\"yourself\",\"yourselves\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7YD0-gPlh2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#finaljson append to extraction\n",
        "final_json={\n",
        "\t\"document\": filename,\n",
        "    \"extraction\": [\n",
        "\t]\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqmWG9JSAbkc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get only tags related to places only to categorize the sentences\n",
        "def ner_for_places_only(sent):\n",
        "  document = client.annotate(sent)\n",
        "  ner_list=[]\n",
        "  placeflag=False\n",
        "  for sent in document.sentence:\n",
        "    for m in sent.mentions:\n",
        "      if m.entityType==\"LOCATION\" or m.entityType==\"COUNTRY\" or m.entityType==\"STATE_OR_PROVINCE\" or m.entityType==\"CITY\":\n",
        "        placeflag=True\n",
        "        ner_list+=[(m.entityMentionText, m.entityType , m.tokenStartInSentenceInclusive ,m.tokenEndInSentenceExclusive)]\n",
        "        \n",
        "\n",
        "  return ner_list,placeflag"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjvNQAgKA5Tb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ner(sent):\n",
        "  # ner_dict = {}\n",
        "  doc = TokenNLP(sent)\n",
        "  # print([(X.start, X.end) for X in doc.ents])\n",
        "  ner_list = [(X.text, X.label_, X.start, X.end) for X in doc.ents]\n",
        "  # ner_list = []\n",
        "  # for X in doc.ents:\n",
        "  #   if X.label_ == \"MONEY\":\n",
        "  #     ner_list.append(X.text)\n",
        "  # TODO custom tag for job titles\n",
        "\n",
        "  return ner_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1PaqGCmAcCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ner_usingstanford_and_job_flag(sent,doyouwantjobsonly=False):\n",
        "  document = client.annotate(sent)\n",
        "  ner_list=[]\n",
        "  jobflag=False\n",
        "  for sent in document.sentence:\n",
        "    for m in sent.mentions:\n",
        "        if doyouwantjobsonly is True:\n",
        "          if(m.entityType==\"TITLE\"):\n",
        "            jobflag=True\n",
        "            ner_list+=[(m.entityMentionText, m.entityType , m.tokenStartInSentenceInclusive ,m.tokenEndInSentenceExclusive)]\n",
        "        else:\n",
        "          ner_list+=[(m.entityMentionText, m.entityType , m.tokenStartInSentenceInclusive ,m.tokenEndInSentenceExclusive)]\n",
        "\n",
        "        if(m.entityType==\"TITLE\"):\n",
        "          jobflag=True\n",
        "        \n",
        "  return ner_list,jobflag"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdrqBTCEAcXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NER MONEY function\n",
        "def get_money123(doc):  \n",
        "  for X in doc.ents:\n",
        "    if X.label_ == \"MONEY\":\n",
        "      return X.text,True\n",
        "\n",
        "  return \"\",False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1-TV4vI8tie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NER Quantity function\n",
        "def get_quantity(doc):  \n",
        "  for X in doc.ents:\n",
        "    if X.label_ == \"QUANTITY\":\n",
        "      return X.text\n",
        "\n",
        "  return \"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb_xH5D1ZyDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this function return the resolved text when it is passed to it:\n",
        "def corefs(text):\n",
        "  x=predictor.predict(document=text)\n",
        "  #print(x)\n",
        "  #occurance of the words\n",
        "  tokens_of_sentence=x['document']\n",
        "  #pronoun->noun dic\n",
        "  noun_coref_instance={}\n",
        "  #get the propernoun:\n",
        "  for initalcluster in x[\"clusters\"]:\n",
        "    #print(initalcluster)\n",
        "    noun_index=None\n",
        "    for mynouns in initalcluster:\n",
        "      y=mynouns\n",
        "      noun_to_use_list=tokens_of_sentence[y[0]:y[1]+1]\n",
        "      noun_to_use=' '.join([str(elem) for elem in noun_to_use_list]) \n",
        "      if noun_to_use.lower() in pronounlist:\n",
        "        continue\n",
        "      else:\n",
        "       \n",
        "        noun_index=y\n",
        "        break\n",
        "\n",
        "      \n",
        "    #print(noun_index)\n",
        "    #make a dictionary of prnouns and the resolved propernoun\n",
        "    for pronouns_index in initalcluster:\n",
        "      y=pronouns_index\n",
        "      pronoun_to_use_list=tokens_of_sentence[y[0]:y[1]+1]\n",
        "      pronoun_to_use=' '.join([str(elem) for elem in pronoun_to_use_list]) \n",
        "      if pronoun_to_use.lower() in pronounlist and noun_index is not None:\n",
        "        noun_coref_instance[tuple(pronouns_index)]=noun_index\n",
        "\n",
        "  #print(noun_coref_instance)\n",
        "\n",
        "  for x in noun_coref_instance:\n",
        "    #print(x)\n",
        "    pronoun_value = tokens_of_sentence[x[0]:x[1]+1]\n",
        "    # print(pronoun_value)\n",
        "    repl_list_strt_idx = x[0]\n",
        "    repl_list_end_idx = x[1]+1\n",
        "    noun_to_use=noun_coref_instance.get(x)\n",
        "    #print(y)\n",
        "    new_list = tokens_of_sentence[noun_to_use[0]:noun_to_use[1]+1]\n",
        "    listToStr = ' '.join([str(elem) for elem in new_list]) \n",
        "    # print(listToStr)\n",
        "    tokens_of_sentence[repl_list_strt_idx : repl_list_end_idx] = [listToStr] \n",
        "    #print(\"----\")\n",
        "\n",
        "    #resolve the dictionary itselfokay wait \n",
        "    \n",
        "  resolved=\"\"\n",
        "  for i in tokens_of_sentence:\n",
        "    resolved+=str(i)+\" \"\n",
        "  return resolved"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCsk5tokZyB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this part of the code resolves the lines/paragraph array with the coref:\n",
        "for x,paragraph in enumerate(lines):\n",
        "  if paragraph==\"\\n\":\n",
        "    continue\n",
        "  print(\"-------------\")\n",
        "  print(paragraph)\n",
        "  \n",
        "  PARAPRO=False\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  docsentence = SentenceNLP(paragraph)\n",
        "  for sent in docsentence.sents:\n",
        "    \n",
        "    doc = nlp(sent.text.strip())\n",
        "    for token in doc: \n",
        "      # If pronoun exists in sentence, activate PRONFLAG\n",
        "      if PARAPRO:\n",
        "        break\n",
        "      if token.pos_ == \"PRON\" or token.text in pronounlist:\n",
        "          PARAPRO = True\n",
        "          print(\"triggered pronoun in para\")\n",
        "          break\n",
        "  if PARAPRO:\n",
        "    resolved_paragraph=corefs(paragraph)\n",
        "    lines[x]=resolved_paragraph\n",
        "\n",
        "    finaldoc = SentenceNLP(lines[x])\n",
        "    for sent in finaldoc.sents:\n",
        "      print(sent.text.strip())\n",
        "      print(\"-------------\")\n",
        "\n",
        "    gotdata=False\n",
        "    try:\n",
        "      nextparatocheck = lines[x+1]\n",
        "      gotdata=True\n",
        "    except IndexError:\n",
        "      gotdata=False\n",
        "      \n",
        "    if gotdata:\n",
        "      finaldoc_next = SentenceNLP(nextparatocheck)\n",
        "      my_sentences_next=[]\n",
        "      for sent in finaldoc_next.sents:\n",
        "        my_sentences_next.append(sent.text.strip())\n",
        "\n",
        "\n",
        "      # To check if Pronoun exists in sentence\n",
        "    \n",
        "      PRONFLAG = False\n",
        "      WORDFLAG = False\n",
        "    \n",
        "      doc = nlp(my_sentences_next[0])\n",
        "      for token in doc: \n",
        "        # If pronoun exists in sentence, activate PRONFLAG\n",
        "        if token.pos_ == \"PRON\" or token.text in pronounlist:\n",
        "            PRONFLAG = True\n",
        "            print(\"triggered pronoun in next sentence\")\n",
        "            break\n",
        "            \n",
        "        if PRONFLAG:\n",
        "          resolvedsentence = corefs(lines[x]+\" mysplitter \"+my_sentences_next[0])\n",
        "          resolvedsentence=resolvedsentence.split(\" mysplitter \")\n",
        "          \n",
        "          my_sentences_next[0]=resolvedsentence[1]\n",
        "          print(resolvedsentence[1])\n",
        "\n",
        "          nextpara=''\n",
        "          for wordsin in my_sentences_next:\n",
        "            nextpara.append(wordsin)\n",
        "          lines[x+1]=nextpara\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF7Y6s-YZxz-",
        "colab_type": "code",
        "outputId": "df0e01e4-2b7a-4a6b-9fde-1348591b7b53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(lines)"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"As the market for personal computers expanded and evolved through the 1990s , Netflix lost market share to the lower - priced duopoly of Microsoft Windows on Intel PC clones . The company board hired CEO Mark Zuckerberg to what would be a 500-day charge for CEO Mark Zuckerberg to rehabilitate the financially troubled company â€” reshaping Netflix with layoffs , executive restructuring , and product focus . In 2001 , CEO Mark Zuckerberg led Netflix to acquire NeXT , solving the desperately failed operating system strategy and bringing Jim back . Jim slowing regained leadership status and became the CEO in 2002 . Netflix swiftly returned to profitability under the revitalizing Think different campaign , as Jim rebuilt Netflix 's status by launching the iMac in 1998 , opening the retail chain of Netflix Stores in 2001 , and acquiring numerous companies to broaden the software portfolio . In January 2007 , Jim renamed the company Netflix Inc. , reflecting Netflix shifted focus toward consumer electronics , and launched the iPhone to great critical acclaim and financial success . In August 2011 , Jim had to resign as CEO due to health complications , and Reed Hastings then became the new CEO . \"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9czzDofmpVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#triggering mechanisms:\n",
        "#going through each paragraph:\n",
        "#lines=[\"In November 2009 , JPMorgan announced JPMorgan would acquire the balance of JPMorgan Cazenove , an advisory and underwriting joint venture established in 2004 with the Cazenove Group , for GBP1 billion .\"]\n",
        "checklist_for_buying=[\"buy\", \"get\", \"purchase\", \"acquire\",\"acquisition\",\"procurement\",\"price\",\"barter\",\"obtain\",\"secure\",\"gain\",\"garner\"]\n",
        "for paragraph in lines:\n",
        "  docsentence = SentenceNLP(paragraph)\n",
        "  for sent in docsentence.sents:\n",
        "    sentence=sent.text.strip()\n",
        "    print(sentence)\n",
        "    doc = TokenNLP(sentence)\n",
        "   \n",
        "    #specify if you want only the job titles\n",
        "    #all types of ner use the flags appropriatly \n",
        "    nerforjobsandflag=ner_usingstanford_and_job_flag(sentence,True)\n",
        "    nerforplacesonly=ner_for_places_only(sentence)\n",
        "    nerusingstanford=ner_usingstanford_and_job_flag(sentence,False)\n",
        "    nerbasic=ner(sentence)\n",
        "    money=get_money123(doc)\n",
        "    quantity=get_quantity(doc)\n",
        "\n",
        "\n",
        "\n",
        "    # print(nerforjobsandflag)\n",
        "    # print(nerforplacesonly)\n",
        "    # print(nerusingstanford)\n",
        "    # print(nerbasic)\n",
        "    # print(money)\n",
        "    # print(quantity)\n",
        "\n",
        "    BuyFlag=False\n",
        "    PlaceFlag=False\n",
        "    OrgFlag=False\n",
        "\n",
        "\n",
        "    #trigger the buy template\n",
        "    for w in checklist_for_buying:\n",
        "      if w in [token.lemma_ for token in doc]:\n",
        "        print(\"processing for buy\")\n",
        "        yo=buytepmlate(sentence)\n",
        "        \n",
        "        for i in yo:\n",
        "          result = {}\n",
        "          result[\"template\"] = \"BUY TEMPLATE\"\n",
        "          result[\"sentence\"] = sentence\n",
        "          result[\"arguments\"] = i\n",
        "          final_json[\"extraction\"].append(result)\n",
        "        break\n",
        "\n",
        "   \n",
        "\n",
        "    if nerforjobsandflag[1]==True:\n",
        "      print(\"trigger job template triggered\")\n",
        "      myrs=worktemp(sentence)\n",
        "      result = {}\n",
        "      for i in myrs:\n",
        "          result[\"template\"] = \"WORK TEMPLATE\"\n",
        "          result[\"sentence\"] = sentence\n",
        "          result[\"arguments\"] = i\n",
        "          final_json[\"extraction\"].append(result)\n",
        "\n",
        "    #trigger the place template \n",
        "    if(nerforplacesonly[1]==True):\n",
        "      print(\"place template triggered\")\n",
        "      rs=place_template(sentence)\n",
        "      for i in rs:\n",
        "          result = {}\n",
        "          result[\"template\"] = \"PLACE TEMPLATE\"\n",
        "          result[\"sentence\"] = sentence\n",
        "          result[\"arguments\"] = i\n",
        "          final_json[\"extraction\"].append(result)\n",
        "\n",
        "\n",
        "    #trigger the Work template\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UYHKNQuR8Ql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "from nltk.corpus import wordnet as wn\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRiMo-5jSPsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NER MONEY function\n",
        "def get_money(doc):  \n",
        "  for X in doc.ents:\n",
        "    if X.label_ == \"MONEY\":\n",
        "      return X.text\n",
        "\n",
        "  return \"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6nuIAmu-E0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BUY function\n",
        "def buytepmlate(sentence):\n",
        "  print(\"FUNC :: buytepmlate()\")\n",
        "\n",
        "  doc = nlp(sentence)\n",
        "\n",
        "  buy_lemmea_list = [\"buy\", \"get\", \"purchase\", \"acquire\",\"acquisition\",\"procurement\",\"price\",\"barter\",\"obtain\",\"secure\",\"gain\",\"garner\"]\n",
        "  for token in doc:\n",
        "    if token.lemma_ in buy_lemmea_list:\n",
        "      vflag = token.pos_ == \"VERB\"\n",
        "\n",
        "  noun_chunks = list(doc.noun_chunks)\n",
        "\n",
        "  sents = list(doc.sents)\n",
        "  root = [s.root.text for s in sents][0]\n",
        "  # print(\"ROOT:\", root)\n",
        "  # print(\"MONEY: \", get_money(doc))\n",
        "  money = get_money(doc)\n",
        "  quant=get_quantity(doc)\n",
        "  for token in doc:\n",
        "    buyer = \"\"\n",
        "    seller_list= []\n",
        "    quantity = \"\"\n",
        "    if token.tag_ == \"VBD\" and token.dep_ == \"ROOT\" and vflag:\n",
        "      # print(\"CASE 1\")\n",
        "      for i, chunk in enumerate(noun_chunks):    \n",
        "        if chunk.root.dep_ == \"nsubj\" and chunk.root.head.text == root:\n",
        "          # print(\"BUYER:\", chunk.text)\n",
        "          buyer = chunk.text \n",
        "        if chunk.root.dep_ == \"dobj\" and chunk.root.head.text == root:\n",
        "          # print(\"SELLER:\", chunk.text)\n",
        "          seller_list.append(chunk.text)\n",
        "        if chunk.root.dep_ == \"appos\" and chunk.root.head.text == noun_chunks[i-1].text.split(\" \")[-1]:\n",
        "          # print(\"SELLER:\", chunk.text)\n",
        "          seller_list.append(chunk.text)\n",
        "\n",
        "      break\n",
        "\n",
        "    # case 2: when lemma is VBN and ROOT\n",
        "    if token.tag_ == \"VBN\" and token.dep_ == \"ROOT\" and vflag:\n",
        "      for chunk in noun_chunks:\n",
        "        if chunk.root.dep_ == \"nsubjpass\" and chunk.root.head.text == root:\n",
        "          # print(\"SELLER:\", chunk.text)\n",
        "          seller_list.append(chunk.text)\n",
        "        if chunk.root.dep_ == \"pobj\" and chunk.root.head.text == \"by\":\n",
        "          # print(\"BUYER:\", chunk.text)\n",
        "          buyer = \"chunk.text\"  \n",
        "      \n",
        "\n",
        "      break\n",
        "    \n",
        "    # case 3: when lemma is NN, find noun chunks\n",
        "    if token.tag_ == \"NN\" and token.dep_ == \"dobj\" and not vflag:\n",
        "      for chunk in noun_chunks:\n",
        "\n",
        "        if chunk.root.dep_ == \"nsubj\" and chunk.root.head.text == root:\n",
        "          # print(\"BUYER:\", chunk.text)\n",
        "          buyer = chunk.text\n",
        "        if chunk.root.dep_ == \"pobj\" and chunk.root.head.text == \"of\":\n",
        "          # print(\"SELLER:\", chunk.text)\n",
        "          seller_list.append(chunk.text)\n",
        "      break\n",
        "\n",
        "  rs = []\n",
        "  for s in seller_list:\n",
        "    res = {}\n",
        "    res[\"buyer\"] = buyer\n",
        "    res[\"source\"] = s\n",
        "    res[\"money\"] = money\n",
        "    res[\"quantity\"] = quant\n",
        "    res[\"item\"]=\"\"\n",
        "\n",
        "    rs.append(res)\n",
        "      \n",
        "  return rs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EgL1umC-FLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PLACE template\n",
        "\n",
        "def place_template(sentence):\n",
        "\n",
        "  subloc = []\n",
        "  suploc = []\n",
        "\n",
        "  doc = nlp(sentence)\n",
        "  # for token in doc:\n",
        "  #   print(token.text, \"-->\", token.lemma_, token.pos_, token.tag_, token.dep_, \"head: \", token.head, token.head.dep_)\n",
        "  #   print(token.text, token.conjuncts)\n",
        "\n",
        "  noun_chunks = list(doc.noun_chunks)\n",
        "  # print(\"\\n---NOUN_CHUNKS---\\n\", noun_chunks)\n",
        "\n",
        "  # for token in doc:\n",
        "  #   if token.pos_ == \"PROPN\" and token. not in noun_chunks:\n",
        "  #     noun_chunks.append(token)\n",
        "\n",
        "  # print(\"chunk.text\", \",\", \"chunk.root.text\", \",\", \"chunk.root.dep_\", \",\", \"chunk.root.head.text\", \", \", \"chunk.root.pos_\", \"chunk.root.tag_\", \"chunk.root.head.dep_\", \"\\nroot's root:\", \"chunk.root.head.head\")\n",
        "  # for chunk in noun_chunks:\n",
        "  #     print(chunk.text, \",\", chunk.root.text, \",\", chunk.root.dep_, \",\", chunk.root.head.text, \", \", chunk.root.pos_, chunk.root.tag_, chunk.root.head.dep_, \"\\nroot's root:\", chunk.root.head.head)\n",
        "\n",
        "\n",
        "  for i, chunk in enumerate(noun_chunks):\n",
        "    if chunk.root.pos_ == \"PROPN\":\n",
        "      # if chunk.root.head.text == \"is\" or chunk.root.head.text == \"are\":\n",
        "      if search_is(chunk.root) and not search_in(chunk.root) and not search_of(chunk.root):\n",
        "        # This part makes \"City 'of' New York work\"\n",
        "        if i+1 < len(noun_chunks) and noun_chunks[i+1].root.head.text == \"of\" and noun_chunks[i+1].root.dep_ == \"pobj\": \n",
        "          # print(\"1SUBLOC:\", chunk.text + \" of \" + noun_chunks[i+1].text, \"i: \", i)\n",
        "          subloc.append(chunk.text + \" of \" + noun_chunks[i+1].text)\n",
        "        else:\n",
        "          # print(\"2SUBLOC:\", chunk.text, \"i: \", i)\n",
        "          subloc.append(chunk.text)\n",
        "\n",
        "\n",
        "    if chunk.root.pos_ == \"PROPN\" or chunk.root.pos_ == \"NOUN\":\n",
        "      if chunk.root.head.text == \"in\":\n",
        "        \n",
        "        if  chunk.root.dep_ == \"pobj\":        \n",
        "          if i+1 < len(noun_chunks) and noun_chunks[i+1].root.head.text == \"of\" and noun_chunks[i+1].root.dep_ == \"pobj\": # and noun_chunks[i+1].root.pos_ == \"PROPN\":\n",
        "            # print(\"4SUPERLOC:\", chunk.text + \" of \" + noun_chunks[i+1].text, \"i: \", i)\n",
        "            suploc.append(chunk.text + \" of \" + noun_chunks[i+1].text)\n",
        "          else:\n",
        "            # print(\"3SUPERLOC:\", chunk.text, \"i: \", i)\n",
        "            suploc.append(chunk.text)\n",
        "\n",
        "\n",
        "      if chunk.root.tag_ == \"NNP\" and chunk.root.dep_ == \"conj\" and chunk.root.head.head.text == \"in\":\n",
        "          # print(\"7SUPERLOC:\", chunk)\n",
        "          suploc.append(chunk.text)\n",
        "    if chunk.root.head.text == \"is\" and chunk.root.head.dep_ == \"relcl\":\n",
        "      # print(\"6SUBLOC::\", noun_chunks[i-1], \"i: \", i)\n",
        "      subloc.append(noun_chunks[i-1])\n",
        "\n",
        "    if chunk.root.head.text == \"of\" and chunk.root.head.head.text == \"part\":\n",
        "      # print(\"5SUPERLOC:\", chunk.text)\n",
        "      suploc.append(chunk.text)\n",
        "\n",
        "    if chunk.root.head.text == \"of\" and chunk.root.head.head.text == \"state\":\n",
        "      # print(\"8SUBLOC:\", chunk.text)\n",
        "      subloc.append(chunk.text)\n",
        "      # print(\"8SUPERLOC:\", noun_chunks[i-1].text[0:-6])\n",
        "      suploc.append(noun_chunks[i-1].text[0:-6])\n",
        "\n",
        "    if chunk.root.pos_ == \"PROPN\" and chunk.root.head.pos_ == \"PROPN\":\n",
        "      # print(\"SUBLOC:\", chunk.root.text)\n",
        "      subloc.append(chunk.root.text)\n",
        "      # print(\"SUPERLOC:\", chunk.root.head.text)\n",
        "      suploc.append(chunk.root.head.text)\n",
        "\n",
        "  result = []\n",
        "  for sp in suploc:\n",
        "    for sb in subloc:\n",
        "      rs = {}\n",
        "      rs[\"super-location\"] = sp\n",
        "      rs[\"sub-location\"] = sb\n",
        "      result.append(rs)\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aofETzF-FdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def worktemp(sentence):\n",
        "  doc = nlp(sentence)\n",
        "\n",
        "  reslist = []\n",
        "  # words = [\"of\", \"at\", \"as\"]\n",
        "  #print(doc)\n",
        "  for token in doc:\n",
        "    if token.text == \"of\" :\n",
        "      res = {}\n",
        "      # print(\"\\n\",token.text, \"::\",\"\\nancestors:\", [anc for anc in token.ancestors], \n",
        "      #       \"\\nchildren:\",[(child, child.pos_) for child in token.children], \"\\nsubtree:\", [s for s in token.subtree],\n",
        "      #       \"\\nlefts:\", [token.text for token in doc[2].lefts])\n",
        "      \n",
        "      #print(\"PERSON:\", \" \".join([token.text for token in doc[2].lefts]))\n",
        "      res[\"person\"] = \" \".join([token.text for token in doc[2].lefts])\n",
        "      \n",
        "      joblist = []\n",
        "      for anc in token.ancestors:\n",
        "        if anc.pos_ in [\"NOUN\", \"PROPN\", \"ADJ\"] and anc.text not in joblist:\n",
        "          joblist.append(anc.text)\n",
        "      #print(\"POSTITION:\", \", \".join(joblist))\n",
        "      res[\"titles\"] = str(joblist)\n",
        "      # print(\"ORG:\",\" \".join([s.text for s in token.subtree][1:]))\n",
        "\n",
        "      sc_flag = False\n",
        "      for i, s in enumerate(token.subtree):\n",
        "        if s.text == \";\":\n",
        "          pst_sc = i\n",
        "          sc_flag = True\n",
        "          break\n",
        "        \n",
        "      if sc_flag:\n",
        "        #print(\"ORG:\",\" \".join([s.text for s in token.subtree][1:pst_sc+1]))\n",
        "        res[\"ORG\"] = \" \".join([s.text for s in token.subtree][1:pst_sc+1])\n",
        "      else:\n",
        "        #print(\"ORG:\",\" \".join([s.text for s in token.subtree][1:]))\n",
        "        res[\"ORG\"] = \" \".join([s.text for s in token.subtree][1:])\n",
        "      #print(\"----\")\n",
        "      reslist.append(res)\n",
        "      # print(\"tag:\", token.tag_, \"pos:\", token.pos_)\n",
        "\n",
        "    if token.text == \"as\":\n",
        "      res = {}\n",
        "      # print(\"\\n\",token.text, \"::\",\"\\nancestors:\", [anc for anc in token.ancestors], \n",
        "      #       \"\\nchildren:\",[(child, child.pos_) for child in token.children], \"\\nsubtree:\", [s for s in token.subtree],\n",
        "      #       \"\\nlefts:\", [token.text for token in doc[2].lefts])\n",
        "      \n",
        "      #print(\"PERSON:\", \" \".join([token.text for token in doc[2].lefts]))\n",
        "      res[\"person\"] = \" \".join([token.text for token in doc[2].lefts])\n",
        "\n",
        "      joblist = []\n",
        "      for anc in token.ancestors:\n",
        "        if anc.pos_ in [\"NOUN\", \"PROPN\", \"ADJ\"] and anc.text not in joblist:\n",
        "          joblist.append(anc.text)\n",
        "      #print(\"POSTITION:\", \", \".join(joblist))\n",
        "      res[\"titles\"] = str(joblist)\n",
        "\n",
        "      of_flag = False\n",
        "      for i, s in enumerate(token.subtree):\n",
        "        if s.text == \"of\":\n",
        "          pst_of = i\n",
        "          of_flag = True\n",
        "        \n",
        "      if of_flag:\n",
        "        #print(\"ORG:\",\" \".join([s.text for s in token.subtree][pst_of+1:]))\n",
        "        res[\"ORG\"] = \" \".join([s.text for s in token.subtree][pst_of+1:])\n",
        "      else:\n",
        "        #print(\"ORG:\",\" \".join([s.text for s in token.subtree][1:]))\n",
        "        res[\"ORG\"] = \" \".join([s.text for s in token.subtree][1:])\n",
        "      #print(\"----\")\n",
        "      reslist.append(rs)\n",
        "  #print(reslist)\n",
        "  return reslist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02B2i5L2SfrV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def search_is(val):\n",
        "  while True:\n",
        "    # print(val)\n",
        "    if val.head.text == \"is\" or val.head.text == \"are\":\n",
        "      return True\n",
        "    elif val.head.text == val.head.head.text:\n",
        "      return False \n",
        "    val = val.head\n",
        "\n",
        "def search_in(val):    \n",
        "  while True:\n",
        "    if val.head.text == \"in\" or val.head.text == \"In\":\n",
        "      return True\n",
        "    elif val.head.text == val.head.head.text:\n",
        "      return False \n",
        "    val = val.head\n",
        "\n",
        "def search_of(val):    \n",
        "  while True:\n",
        "    if val.head.text == \"of\":\n",
        "      return True\n",
        "    elif val.head.text == val.head.head.text:\n",
        "      return False \n",
        "    val = val.head\n",
        "\n",
        "def search_including(val):    \n",
        "  while True:\n",
        "    if val.head.text == \"including\" or val.head.text == \"including\":\n",
        "      return True\n",
        "    elif val.head.text == val.head.head.text:\n",
        "      return False \n",
        "    val = val.head"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWMcpixgg_f3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcTsPXmHjaiD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io, json\n",
        "print(final_json)\n",
        "json_object = json.dumps(final_json, indent = 3) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khQrfhl6qqUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "with io.open(filename+'_output.json', 'w') as f:\n",
        "  f.write(json_object)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}